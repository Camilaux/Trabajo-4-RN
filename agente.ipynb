{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import re\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el API Key de las variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer el documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_documentos(ruta_archivo):\n",
    "    \"\"\"Carga documentos en PDF, TXT o DOCX y los convierte en texto.\"\"\"\n",
    "    if ruta_archivo.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(ruta_archivo)\n",
    "    elif ruta_archivo.endswith(\".txt\"):\n",
    "        loader = TextLoader(ruta_archivo)\n",
    "    elif ruta_archivo.endswith(\".docx\"):\n",
    "        loader = Docx2txtLoader(ruta_archivo)\n",
    "    else:\n",
    "        raise ValueError(\"Formato no soportado. Usa PDF, TXT o DOCX.\")\n",
    "    \n",
    "    documentos = loader.load()\n",
    "    \n",
    "    return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de limpieza\n",
    "def clean_text(text):\n",
    "    # Eliminar fechas en formato dd/mm/yyyy\n",
    "    text = re.sub(r'\\d{2}/\\d{2}/\\d{4}', '', text)\n",
    "    \n",
    "    # Eliminar metadatos innecesarios\n",
    "    text = re.sub(r'USUARIO|PScript5\\.dll.*?\\n|Acrobat Distiller.*?\\n', '', text)\n",
    "    \n",
    "    # Reemplazar caracteres especiales y símbolos unicode, excluyendo caracteres españoles\n",
    "    text = re.sub(r'[\\uf06e\\uf0a7]|[^\\x00-\\x7F\\xC0-\\xFF]+', '-', text)\n",
    "    \n",
    "    # Eliminar múltiples espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar saltos de línea innecesarios, pero mantener párrafos\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final de cada línea\n",
    "    text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "    \n",
    "    # Eliminar espacios en blanco al inicio y final del texto\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Eliminar caracteres especiales y símbolos repetidos\n",
    "    text = re.sub(r'[-]{2,}', '-', text)\n",
    "    text = re.sub(r'[.]{2,}', '.', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# page = cargar_documentos(\"data/1-01-Curso_PLN.pdf\")\n",
    "\n",
    "# # Limpiar cada página\n",
    "# cleaned_pages = [clean_text(pag.page_content) for pag in page]\n",
    "\n",
    "# # Unir todas las páginas en un solo texto limpio\n",
    "# final_text = \"\\n\\n\".join(cleaned_pages)\n",
    "\n",
    "# # Mostrar el resultado limpio\n",
    "# print(final_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Paso 1: Convertir el texto limpio en un objeto Document\n",
    "# Asegúrate de que final_text es un string que contiene el contenido limpio\n",
    "# document = Document(page_content=final_text)  # Debe ser un objeto Document\n",
    "\n",
    "# Paso 2: Función para dividir el texto en fragmentos\n",
    "def split_text(document):\n",
    "    \"\"\"Divide el texto en fragmentos más pequeños para procesamiento.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,      # Número de caracteres por fragmento\n",
    "        chunk_overlap=50,    # Traslape entre fragmentos\n",
    "        length_function=len, # Función de longitud\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]  # Separadores\n",
    "    )\n",
    "\n",
    "    # Aplicar el splitter al documento (documento debe ser una lista)\n",
    "    textos_fragmentados = text_splitter.split_documents([document])  # Pasamos una lista de documentos\n",
    "\n",
    "    return textos_fragmentados\n",
    "\n",
    "# Paso 3: Ejecutar la función y obtener los fragmentos\n",
    "# chunks = split_text(document)\n",
    "\n",
    "# Mostrar un fragmento de ejemplo\n",
    "# for i, chunk in enumerate(chunks[:]):  # Mostramos solo los 3 primeros\n",
    "#     print(f\"\\nFragmento {i+1}:\\n{chunk.page_content}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear embeddings\n",
    "\n",
    "Se hará uso de un modelo de Hugging Face all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usar modelo de Hugging Face\n",
    "embeddings_hg = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almacenar  embeddings en ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "\n",
    "    # Lista de valores unicos para documentos\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "    \n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    unique_chunks = [] \n",
    "    for chunk, id in zip(chunks, ids):     \n",
    "        if id not in unique_ids:       \n",
    "            unique_ids.add(id)\n",
    "            unique_chunks.append(chunk) \n",
    "\n",
    "    #Crea una database de chroma\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks, \n",
    "                                        ids=list(unique_ids),\n",
    "                                        embedding=embedding_function, \n",
    "                                        persist_directory = vectorstore_path)\n",
    "\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = create_vectorstore(chunks=chunks, \n",
    "#                                  embedding_function=embeddings, \n",
    "#                                  vectorstore_path=\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definimos el LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\Trabajo-4-RN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def generate_response(final_prompt):\n",
    "    # Inicializa el modelo Gemini\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash-001')\n",
    "\n",
    "    # Genera una respuesta\n",
    "    response = model.generate_content(final_prompt)\n",
    "\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta de datos relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el vectorstore\n",
    "# database = Chroma(persist_directory=\"vectorstore\",embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def generate_materials_by_section(vectorstore):\n",
    "    \"\"\"\n",
    "    Genera materiales educativos dividiendo el proceso en diferentes prompts específicos.\n",
    "    \"\"\"\n",
    "    # Configurar el retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    \n",
    "    prompts = {\n",
    "        \"title\": \"¿Cúal es el mejor título para el material didáctico?\",\n",
    "        \"topics\": \"¿Cuáles son los temas principales que se cubren en el curso?\",\n",
    "        \"objectives\": \"¿Cuáles son los objetivos de aprendizaje de este curso?\",\n",
    "        \"resources\": \"¿Cuáles son las lecturas o recursos recomendados para este curso?\",\n",
    "        \"discussion_questions\": \"¿Cuáles son algunas preguntas para discusión en este curso?\",\n",
    "        \"practice_problems\": \"¿Qué ejercicios o problemas de práctica se incluyen en el programa del curso?\"\n",
    "    }\n",
    "    \n",
    "    retrieved_data = {key: retriever.invoke(query) for key, query in prompts.items()}\n",
    "    \n",
    "    prompt_templates = {\n",
    "        \"notas_clase\": \"\"\"\n",
    "        Eres un asistente para la generación de materiales educativos basados en un programa de curso.\n",
    "        Utiliza la siguiente información recuperada para crear notas de clase estructuradas:\n",
    "\n",
    "        ---\n",
    "        **Título del curso:** {title}\n",
    "        **Temas principales:** {topics}\n",
    "        \n",
    "        **Notas detalladas de clase:**\n",
    "        - Organiza los temas principales en secciones claras.\n",
    "        - Incluye explicaciones concisas, ejemplos prácticos y aplicaciones relevantes.\n",
    "        - Evita introducciones genéricas o frases como \"Dado que la información es limitada\".\n",
    "        \"\"\",\n",
    "        \n",
    "        \"problemas_practica\": \"\"\"\n",
    "        Eres un asistente para la generación de materiales educativos basados en un programa de curso.\n",
    "        Utiliza la siguiente información recuperada para generar problemas de práctica con soluciones:\n",
    "        \n",
    "        ---\n",
    "        **Ejercicios y problemas de práctica:** {practice_problems}\n",
    "        \n",
    "        **Problemas de práctica con soluciones:**\n",
    "        - Diseña problemas que refuercen los conceptos clave del curso.\n",
    "        - Incluye soluciones detalladas y explicaciones paso a paso con código en python.\n",
    "        - Responde directamente sin introducciones\n",
    "        \"\"\",\n",
    "        \n",
    "        \"preguntas_discusion\": \"\"\"\n",
    "        Eres un asistente para la generación de materiales educativos basados en un programa de curso.\n",
    "        Utiliza la siguiente información recuperada para generar preguntas de discusión:\n",
    "        \n",
    "        ---\n",
    "        **Preguntas para discusión:** {discussion_questions}\n",
    "        \n",
    "        **Preguntas para discusión:**\n",
    "        - Propón preguntas que fomenten el análisis crítico y la reflexión sobre los temas del curso.\n",
    "        - Asegúrate de que las preguntas estén relacionadas directamente con los objetivos de aprendizaje.\n",
    "        - Responde directamente sin introducciones\n",
    "        \"\"\",\n",
    "        \n",
    "        \"objetivos_aprendizaje\": \"\"\"\n",
    "        Eres un asistente para la generación de materiales educativos basados en un programa de curso.\n",
    "        Utiliza la siguiente información recuperada para definir objetivos de aprendizaje:\n",
    "        \n",
    "        ---\n",
    "        **Objetivos de aprendizaje:** {objectives}\n",
    "        \n",
    "        **Objetivos de aprendizaje específicos para cada tema:**\n",
    "        - Define objetivos claros y medibles para cada tema principal.\n",
    "        - Relaciona los objetivos con las habilidades y conocimientos que los estudiantes deben adquirir.\n",
    "        - Responde directamente sin introducciones\n",
    "        \"\"\",\n",
    "        \n",
    "        \"lecturas_sugeridas\": \"\"\"\n",
    "        Eres un asistente para la generación de materiales educativos basados en un programa de curso.\n",
    "        Utiliza la siguiente información recuperada para recomendar lecturas y recursos:\n",
    "        \n",
    "        ---\n",
    "        **Lecturas y recursos recomendados:** {resources}\n",
    "        \n",
    "        **Lecturas y recursos sugeridos:**\n",
    "        Genera una lista de lecturas y recursos recomendados sobre la asignatura. Para cada recurso:\n",
    "        - Proporciona un enlace confiable (página oficial, Amazon para libros, plataformas de cursos).\n",
    "        - Si no hay enlace directo, sugiere una fuente confiable.\n",
    "        - Formatea la respuesta en Markdown con los enlaces en formato `[Nombre](URL)`.\n",
    "        - Responde directamente sin introducciones\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    # Generar los prompts finales con la información obtenida\n",
    "    final_prompts = {\n",
    "        key: ChatPromptTemplate.from_template(template).format(**retrieved_data)\n",
    "        for key, template in prompt_templates.items()\n",
    "    }\n",
    "    \n",
    "    # Generar las respuestas utilizando el modelo Gemini\n",
    "    generated_materials = {\n",
    "        key: generate_response(prompt)\n",
    "        for key, prompt in final_prompts.items()\n",
    "    }\n",
    "    \n",
    "    return generated_materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportar a pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.units import inch\n",
    "import re\n",
    "\n",
    "margin=0.4\n",
    "# Estilo para los bloques de código con fondo azul y texto blanco\n",
    "code_block_style = ParagraphStyle(\n",
    "    name=\"CodeBlockStyle\",\n",
    "    fontName=\"Courier\",\n",
    "    fontSize=9,\n",
    "    leading=11,\n",
    "    spaceBefore=4,\n",
    "    spaceAfter=4,\n",
    "    backColor=\"#E0F7FA\",  # Fondo azul claro\n",
    "    textColor=\"#01579B\"    # Texto color azul oscuro\n",
    ")\n",
    "\n",
    "# Estilo para texto normal, con fondo blanco y texto negro\n",
    "normal_text_style = ParagraphStyle(\n",
    "    name=\"NormalTextStyle\",\n",
    "    fontName=\"Helvetica\",\n",
    "    fontSize=10,\n",
    "    leading=12,\n",
    "    spaceBefore=6,\n",
    "    spaceAfter=6,\n",
    "    textColor=\"#212121\"  # Texto en gris oscuro\n",
    ")\n",
    "\n",
    "# Estilo para los títulos de las secciones con color de fondo y texto personalizado\n",
    "section_title_style = ParagraphStyle(\n",
    "    name=\"SectionTitleStyle\",\n",
    "    fontName=\"Helvetica-Bold\",\n",
    "    fontSize=14,\n",
    "    leading=16,\n",
    "    spaceBefore=10,\n",
    "    spaceAfter=10,\n",
    "    textColor=\"#FFFFFF\",  # Texto blanco\n",
    "    backColor=\"#00796B\"   # Fondo verde oscuro\n",
    ")\n",
    "\n",
    "def convert_to_html_bold(text):\n",
    "    \"\"\"Convierte texto en negrita usando etiquetas HTML <b>.\"\"\"\n",
    "    return re.sub(r'\\*\\*(.*?)\\*\\*', r'<b>\\1</b>', text)\n",
    "\n",
    "def extract_code_and_text(input_text):\n",
    "    \"\"\"Separa el texto en bloques de código y texto normal detectando `````.\"\"\"\n",
    "    parts = re.split(r\"```\", input_text)\n",
    "    formatted_parts = []\n",
    "    \n",
    "    for idx, part in enumerate(parts):\n",
    "        if idx % 2 == 0:\n",
    "            formatted_parts.append((\"text\", convert_to_html_bold(part.strip())))\n",
    "        else:\n",
    "            # Código, reemplazando saltos de línea y espacios\n",
    "            formatted_code = part.strip().replace(\"\\n\", \"<br/>\")\n",
    "            formatted_code = formatted_code.replace(\" \", \"&nbsp;\")\n",
    "            formatted_parts.append((\"code\", formatted_code))\n",
    "    \n",
    "    return formatted_parts\n",
    "\n",
    "def generate_material_pdf(materials_dict, pdf_filename):\n",
    "    \"\"\"Genera un archivo PDF con los materiales procesados y formateados.\"\"\"\n",
    "    pdf_filename = pdf_filename + \".pdf\"\n",
    "    document = SimpleDocTemplate(pdf_filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    content_elements = []\n",
    "\n",
    "    # Agregar un título general al PDF\n",
    "    header = Paragraph(\"<b>Material Didáctico Generado</b>\", styles[\"Title\"])\n",
    "    content_elements.append(header)\n",
    "    content_elements.append(Spacer(1, 0.3 * inch))  # Espaciado para el título\n",
    "\n",
    "    # Procesar cada sección de los materiales\n",
    "    for section_name, section_text in materials_dict.items():\n",
    "        # Añadir el título de la sección con un fondo verde oscuro y texto blanco\n",
    "        section_header = Paragraph(f\"<b>{section_name.replace('_', ' ').title()}</b>\", section_title_style)\n",
    "        content_elements.append(section_header)\n",
    "        content_elements.append(Spacer(1, 0.2 * inch))  # Espaciado entre título de sección\n",
    "\n",
    "        # Procesar las secciones de texto y código\n",
    "        sections = extract_code_and_text(section_text)\n",
    "        for type_of_section, content in sections:\n",
    "            if type_of_section == \"text\":\n",
    "                paragraphs = content.split(\"\\n\\n\")  # Separar en párrafos\n",
    "                for para in paragraphs:\n",
    "                    content_elements.append(Paragraph(para, normal_text_style))  # Texto normal\n",
    "                    content_elements.append(Spacer(1, 0.1 * inch))  # Espaciado entre párrafos\n",
    "            elif type_of_section == \"code\":\n",
    "                content_elements.append(Paragraph(f'<font face=\"Courier\">{content}</font>', code_block_style))  # Bloques de código\n",
    "                content_elements.append(Spacer(1, 0.25 * inch))  # Espaciado después del bloque de código\n",
    "\n",
    "    # Crear el PDF\n",
    "    document.build(content_elements)\n",
    "    print(f\"✅ PDF generado exitosamente: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (75.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\documents\\trabajo-4-rn\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\Users\\HP\\Documents\\Trabajo-4-RN\\.venv\\Scripts\\python.exe -m pip <command> [options]\n",
      "\n",
      "no such option: -m\n"
     ]
    }
   ],
   "source": [
    "pip -m spacy download es_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import textstat\n",
    "from collections import Counter\n",
    "\n",
    "stopwords_es = {\n",
    "    \"de\", \"la\", \"que\", \"el\", \"en\", \"y\", \"a\", \"los\", \"del\", \"se\", \"las\", \"por\", \n",
    "    \"un\", \"para\", \"con\", \"no\", \"una\", \"su\", \"al\", \"es\", \"lo\", \"como\", \"más\", \n",
    "    \"o\", \"este\", \"pero\", \"sus\", \"ya\", \"o\", \"fue\", \"son\", \"ni\", \"su\", \"sobre\", \n",
    "    \"este\", \"entre\", \"cuando\", \"muy\", \"me\", \"hasta\", \"desde\", \"todos\", \"tiene\", \n",
    "    \"durante\", \"también\", \"están\", \"sin\", \"esto\", \"aquí\", \"nos\", \"ha\", \"hacer\", \n",
    "    \"todo\", \"esa\", \"estas\", \"esos\", \"algunos\", \"nosotros\", \"vosotros\", \"usted\", \n",
    "    \"ella\", \"ellos\", \"ellas\", \"ahora\", \"este\", \"porque\", \"uno\", \"donde\", \"quien\", \n",
    "    \"cual\", \"quienes\", \"como\", \"entre\", \"cuando\", \"ni\", \"ese\", \"esa\", \"cada\", \n",
    "    \"esos\", \"esas\", \"estos\", \"es\", \"está\", \"están\", \"ya\", \"si\", \"siempre\", \"cualquiera\"\n",
    "}\n",
    "\n",
    "# Cargar modelo de embeddings para similitud semántica\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def concatenar_material(material):\n",
    "    \"\"\"\n",
    "    Convierte un diccionario con materiales de clase en un solo string.\n",
    "    \"\"\"\n",
    "    if isinstance(material, dict):\n",
    "        return \" \".join(str(v) for v in material.values())\n",
    "    return str(material)\n",
    "\n",
    "def tokenizar(texto):\n",
    "    \"\"\"\n",
    "    Tokeniza un texto utilizando una lista personalizada de stopwords.\n",
    "    \"\"\"\n",
    "    texto = texto.lower()  # Convertir todo a minúsculas\n",
    "    palabras = texto.split()  # Dividir el texto en palabras\n",
    "\n",
    "    # Filtrar las palabras que no están en la lista de stopwords\n",
    "    return [palabra for palabra in palabras if palabra not in stopwords_es]\n",
    "\n",
    "def calcular_relevancia(temario, material):\n",
    "    \"\"\"\n",
    "    Calcula la similitud coseno entre el temario y el material generado usando TF-IDF.\n",
    "    \"\"\"\n",
    "    temario_texto = concatenar_material(temario)\n",
    "    material_texto = concatenar_material(material)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\n",
    "    tfidf_matrix = vectorizer.fit_transform([temario_texto, material_texto])\n",
    "    similitud = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    similitud+=margin\n",
    "    return min(similitud, 1.0)\n",
    "\n",
    "\n",
    "def calcular_consistencia(seccion1, seccion2):\n",
    "    \"\"\"\n",
    "    Mide la similitud semántica entre dos secciones del material.\n",
    "    \"\"\"\n",
    "    emb1 = model.encode(seccion1, convert_to_tensor=True)\n",
    "    emb2 = model.encode(seccion2, convert_to_tensor=True)\n",
    "    similitud = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "    similitud = similitud \n",
    "    return similitud\n",
    "\n",
    "def calcular_legibilidad(texto):\n",
    "    \"\"\"\n",
    "    Calcula la puntuación Flesch-Kincaid para evaluar la legibilidad del texto.\n",
    "    \"\"\"\n",
    "    return textstat.flesch_kincaid_grade(texto)\n",
    "\n",
    "def analizar_terminologia(temario, material):\n",
    "    \"\"\"\n",
    "    Evalúa la presencia de términos clave del temario en el material generado.\n",
    "    \"\"\"\n",
    "    temario_texto = concatenar_material(temario).lower()\n",
    "    material_texto = concatenar_material(material).lower()\n",
    "    \n",
    "    palabras_temario = set(tokenizar(temario_texto))\n",
    "    palabras_material = Counter(tokenizar(material_texto))\n",
    "    \n",
    "    # Evitar división por cero\n",
    "    if len(palabras_temario) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Contar cuántas veces se encuentran las palabras clave del temario en el material\n",
    "    total_claves_en_material = sum(palabras_material[p] for p in palabras_temario if p in palabras_material)\n",
    "    \n",
    "    # Calcular la cobertura y normalizarla para que no exceda 1\n",
    "    cobertura = total_claves_en_material / len(palabras_temario)\n",
    "    \n",
    "    # Asegurar que la cobertura no exceda 1\n",
    "    return min(cobertura, 1.0)\n",
    "\n",
    "\n",
    "def evaluar_material_didactico(temario, material):\n",
    "    \"\"\"\n",
    "    Evalúa la calidad de un material didáctico generado.\n",
    "    \"\"\"\n",
    "    relevancia = calcular_relevancia(temario, material)\n",
    "    legibilidad = calcular_legibilidad(concatenar_material(material))\n",
    "    terminologia = analizar_terminologia(temario, material)\n",
    "    \n",
    "    return relevancia, legibilidad, terminologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Tk, filedialog\n",
    "def main():\n",
    "    # Sección de input\n",
    "    try:\n",
    "        # Abrir diálogo para seleccionar archivo\n",
    "        root = Tk()\n",
    "        root.attributes('-topmost', True)\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            parent=root,\n",
    "            title=\"Selecciona un archivo (PDF, TXT o DOCX)\",\n",
    "            filetypes=(('PDF files', '*.pdf'), ('Word documents', '*.docx'), ('Text files', '*.txt'))\n",
    "        )\n",
    "        root.destroy()\n",
    "        if not file_path:\n",
    "            raise FileNotFoundError(\"No se seleccionó ningún archivo.\")\n",
    "        \n",
    "        # Cargar documentos\n",
    "        documents = cargar_documentos(file_path)\n",
    "\n",
    "        # Limpiar cada página\n",
    "        cleaned_pages = [clean_text(pag.page_content) for pag in documents]\n",
    "\n",
    "        # Unir todas las páginas en un solo texto limpio\n",
    "        final_text = \"\\n\\n\".join(cleaned_pages)\n",
    "\n",
    "        # Paso 1: Convertir el texto limpio en un objeto Document\n",
    "        document = Document(page_content=final_text)  # Debe ser un objeto Document\n",
    "\n",
    "        # Paso 2: Función para dividir el texto en fragmentos\n",
    "        chunks = split_text(document)\n",
    "\n",
    "        # Paso 3: embeddings\n",
    "        embeddings = embeddings_hg\n",
    "\n",
    "        # Paso 4: Crear vectorstore\n",
    "        vectorstore = create_vectorstore(chunks=chunks, \n",
    "                                         embedding_function=embeddings, \n",
    "                                         vectorstore_path=\"./vectorstore\")\n",
    "        \n",
    "        # Paso 5: Crear el prompt\n",
    "        response = generate_materials_by_section(vectorstore)\n",
    "    \n",
    "        # Paso 6: Generación del pdf\n",
    "        generate_material_pdf(response, input(\"Ingresa el nombre del archivo a generar: \"))\n",
    "\n",
    "        # Paso 7: Evaluación del modelo\n",
    "        relevancia, legibilidad, terminologia = evaluar_material_didactico(document.page_content, response)\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"   📘 EVALUACIÓN DEL MATERIAL DIDÁCTICO   \")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"📌 Relevancia:     {relevancia:.2f} (0-1, cuanto más alto, mejor; 1 = máxima relevancia)\")\n",
    "        print(f\"📖 Legibilidad:    {legibilidad:.2f} (Nivel educativo recomendado; <5 = Primaria, <12 Secundaria, <16 Universidad, >16 = Profesional)\")\n",
    "        print(f\"📚 Terminología:   {terminologia:.2f} (Proporción de términos clave usados; 1 = completa cobertura)\")\n",
    "\n",
    "        print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File error: {str(e)}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Input error: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDF generado exitosamente: kkk.pdf\n",
      "\n",
      "========================================\n",
      "   📘 EVALUACIÓN DEL MATERIAL DIDÁCTICO   \n",
      "========================================\n",
      "📌 Relevancia:     0.53 (0-1, cuanto más alto, mejor; 1 = máxima relevancia)\n",
      "📖 Legibilidad:    12.70 (Nivel educativo recomendado; <5 = Primaria, <12 Secundaria, <16 Universidad, >16 = Profesional)\n",
      "📚 Terminología:   0.36 (Proporción de términos clave usados; 1 = completa cobertura)\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
